{"cells":[{"cell_type":"markdown","metadata":{"id":"iiB2f3Y-5eXS"},"source":["# Information Retrieval and Web Analytics\n","\n","# Indexing + Modeling (TF-IDF)"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"YVfEcBBOZUbK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666358645238,"user_tz":-120,"elapsed":416314,"user":{"displayName":"CARLA VEGA MOLINA","userId":"01741420367521501300"}},"outputId":"043f611b-0d10-47a5-fe58-aabb102ab85d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"iNt6V-hu5eXa"},"source":["#### Load Python packages\n","Let's first import all the packages that you will need during this assignment."]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z27uXzYa5eXa","outputId":"37f23053-b8a4-4c37-9c3f-b27b96cd0389","scrolled":true,"executionInfo":{"status":"ok","timestamp":1666358646733,"user_tz":-120,"elapsed":1499,"user":{"displayName":"CARLA VEGA MOLINA","userId":"01741420367521501300"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]}],"source":["import nltk\n","nltk.download('stopwords')\n","from collections import defaultdict\n","from array import array\n","from nltk.stem import PorterStemmer\n","from nltk.corpus import stopwords\n","import math\n","import numpy as np\n","import collections\n","from numpy import linalg as la\n","import string\n","import json\n","import re"]},{"cell_type":"markdown","metadata":{"id":"JdOqU3nR5eXc"},"source":["#### Load data into memory\n","The dataset is stored in the json file, and it contains 4,000 tweets related to Hurrican Ian. Each line readed represents a tweet, and for each tweet we have its corresponding information."]},{"cell_type":"code","execution_count":3,"metadata":{"id":"9a5z65YP5eXd","executionInfo":{"status":"ok","timestamp":1666358648482,"user_tz":-120,"elapsed":1752,"user":{"displayName":"CARLA VEGA MOLINA","userId":"01741420367521501300"}}},"outputs":[],"source":["docs_path = 'drive/Shareddrives/IR/Project/data/tw_hurricane_data.json'\n","with open(docs_path) as fp:\n","    lines = fp.readlines()"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1urImOuK5eXe","outputId":"63dc6273-3756-44be-be6d-fda326910dc1","executionInfo":{"status":"ok","timestamp":1666358648482,"user_tz":-120,"elapsed":6,"user":{"displayName":"CARLA VEGA MOLINA","userId":"01741420367521501300"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Total number of tweets: 4000\n"]}],"source":["print(\"Total number of tweets: {}\".format(len(lines)))"]},{"cell_type":"markdown","metadata":{"id":"7c21sBGa5eXe"},"source":["First, we will implement the mandatory preprocessing in the function ```clean_tweet(line)```.\n","\n","It will take as an input a line, and it will:\n","\n","- Transform into lower case\n","- Remove punctiation marks\n","- Tokenize the text to get a list of terms (*split function*)\n","- Remove stop words\n","- Stem terms (example: to stem the term 'researcher', we will use ```stemming.stem(researcher)```)\n","\n","The extra tasks such as removing emojis "]},{"cell_type":"code","execution_count":5,"metadata":{"id":"SeYKSVnH5eXf","executionInfo":{"status":"ok","timestamp":1666358648483,"user_tz":-120,"elapsed":5,"user":{"displayName":"CARLA VEGA MOLINA","userId":"01741420367521501300"}}},"outputs":[],"source":["def clean_tweet(line, emojis):\n","    \"\"\"\n","    Preprocess the article text (title + body) removing stop words, stemming,\n","    transforming in lowercase and return the tokens of the text.\n","    \n","    Argument:\n","    line -- string (text) to be preprocessed\n","    \n","    Returns:\n","    line - a list of tokens corresponding to the input text after the preprocessing\n","    \"\"\"\n","\n","    line=str(line)  # by default everything assumed as string\n","\n","    line = line.lower()  # transform into lower case\n","\n","    line = re.sub(r'\\\\n', ' ', line)  #remove new lines\n","\n","    line = line.translate(str.maketrans('', '', string.punctuation))  # remove punctuation marks\n","\n","    line = re.sub(r'http\\S+', '', line)  # remove links\n","\n","    if emojis: # remove emojis\n","        ### https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b\n","        remove_emojis = re.compile(\"[\"\n","                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n","                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n","                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n","                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n","                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n","                               u\"\\U00002702-\\U000027B0\"\n","                               u\"\\U00002702-\\U000027B0\"\n","                               u\"\\U000024C2-\\U0001F251\"\n","                               u\"\\U0001f926-\\U0001f937\"\n","                               u\"\\U00010000-\\U0010ffff\"\n","                               u\"\\u2640-\\u2642\"\n","                               u\"\\u2600-\\u2B55\"\n","                               u\"\\u200d\"\n","                               u\"\\u23cf\"\n","                               u\"\\u23e9\"\n","                               u\"\\u231a\"\n","                               u\"\\ufe0f\"  # dingbats\n","                               u\"\\u3030\"\n","                               u\"\\u2026\" # ...\n","                               u\"\\u2019\" # '\n","                               u\"\\u2066\"\n","                               u\"\\u2069\"\n","                               u\"\\u231b\"\n","                               \"]+\", flags=re.UNICODE)\n","        line = remove_emojis.sub(r'', str(line))\n","\n","                     \n","    line = line.split()  # Tokenize the text to get a list of terms\n","\n","    stop_words = set(stopwords.words(\"english\")) # remove the stopwords\n","    line = [x for x in line if x not in stop_words]  \n","\n","    stemmer = PorterStemmer() #stem terms\n","    line = [stemmer.stem(word) for word in line]    \n","        \n","    return line"]},{"cell_type":"code","source":["def process_json_line(json_line):   #given a json line iterates throuhg its content and creates another json with values formated\n","\n","  new_dict = {}\n","\n","  index_stored = 0\n","  for key, value in json_line.items():\n","\n","    if (key==\"full_text\"): new_dict[\"Tweet\"] = clean_tweet(value, True) #process tweet\n","\n","    elif(key==\"created_at\"): new_dict[\"Date\"] = clean_tweet(value, False) #process tweet date      \n","\n","    elif(key==\"retweet_count\"): new_dict[\"Retweets\"] = value #number of retweet, no need of processing\n","\n","    elif(key==\"favorite_count\"): new_dict[\"Likes\"] = value #number of likes, no need of processing\n","    elif(key==\"id\"): new_dict[\"Tweet_id\"] = [str(value)] #store tweet id as str\n","\n","    elif(key==\"user\"): new_dict[\"Username\"] = clean_tweet(value[\"screen_name\"], True) #process username\n","\n","    elif(key==\"entities\"): #process hashtags\n","      hashtags=[]\n","      for i in range(len(value[\"hashtags\"])):\n","        hashtags.append(clean_tweet(value[\"hashtags\"][i][\"text\"], False))\n","      new_dict[\"Hashtags\"] = hashtags\n","\n","  new_dict[\"Url\"] = 'https://twitter.com/'+ new_dict[\"Username\"][0] + '/status/' + str(new_dict[\"Tweet_id\"][0]) #generate url\n","\n","      \n","  return new_dict"],"metadata":{"id":"4dY4pLD4VSBP","executionInfo":{"status":"ok","timestamp":1666358648483,"user_tz":-120,"elapsed":5,"user":{"displayName":"CARLA VEGA MOLINA","userId":"01741420367521501300"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","execution_count":7,"metadata":{"id":"PE7_Rq4D5eXk","executionInfo":{"status":"ok","timestamp":1666358655223,"user_tz":-120,"elapsed":6744,"user":{"displayName":"CARLA VEGA MOLINA","userId":"01741420367521501300"}}},"outputs":[],"source":["json_processed=[]\n","\n","for line in lines:\n","\n","  data=json.loads(line)\n","  \n","  json_line=process_json_line(data) #format json\n","\n","  json_processed.append(json_line) #store line"]},{"cell_type":"markdown","source":["#### Mapping ids\n","Mapping tweets IDs with the document IDS."],"metadata":{"id":"lqF3-VibYWR1"}},{"cell_type":"code","source":["import csv\n","mapping_dict = {} # create empty dictionary to store the documents id\n","\n","docs_path = 'drive/Shareddrives/IR/Project/data/tweet_document_ids_map.csv'\n","csv_file = csv.reader(open(docs_path, 'rU'), delimiter=\"\\t\", quotechar='|') # open csv\n","\n","for line in csv_file: mapping_dict[line[1]] = line[0]   # store the indexes in the dictionary (tweet id, document id)\n","\n","for j in json_processed:\n","  m = []\n","  m.append(str(mapping_dict[j['Tweet_id'][0]]))\n","  j['doc_id'] = m   # add the document id in the json dictionary"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d4W0VORm9Rqi","executionInfo":{"status":"ok","timestamp":1666358655223,"user_tz":-120,"elapsed":8,"user":{"displayName":"CARLA VEGA MOLINA","userId":"01741420367521501300"}},"outputId":"4be73f79-a1ea-46ac-e060-045bff26b6ff"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: 'U' mode is deprecated\n","  \"\"\"\n"]}]},{"cell_type":"code","source":["json_processed[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qI6TBJd7YnRY","executionInfo":{"status":"ok","timestamp":1666358655666,"user_tz":-120,"elapsed":448,"user":{"displayName":"CARLA VEGA MOLINA","userId":"01741420367521501300"}},"outputId":"6f78dbcb-ed35-434c-c30a-ca57fd32e550"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'Date': ['fri', 'sep', '30', '183908', '0000', '2022'],\n"," 'Tweet_id': ['1575918182698979328'],\n"," 'Tweet': ['keep',\n","  'spin',\n","  'us',\n","  '7',\n","  'pmgo',\n","  'away',\n","  'alreadi',\n","  'hurricaneian'],\n"," 'Hashtags': [['hurricaneian']],\n"," 'Username': ['suzjdean'],\n"," 'Retweets': 0,\n"," 'Likes': 0,\n"," 'Url': 'https://twitter.com/suzjdean/status/1575918182698979328',\n"," 'doc_id': ['doc_1']}"]},"metadata":{},"execution_count":9}]}],"metadata":{"colab":{"provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"nbformat":4,"nbformat_minor":0}